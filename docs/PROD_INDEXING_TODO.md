# üîß Prod Indexering - TODO

## Problemet

**Lokalt:**
- ‚úÖ Dokument indexeras n√§r du k√∂r `scripts/index_workspace.py`
- ‚úÖ Embeddings genereras, BM25-index byggs
- ‚úÖ Index cache sparas till disk
- ‚úÖ CLI funkar 10/10

**Prod (Railway):**
- ‚ùå `/documents/upload` sparar bara till R2 och DB
- ‚ùå Ingen indexering sker (extract ‚Üí chunk ‚Üí embed ‚Üí save index)
- ‚ùå `/health` visar `indexed_chunks: 0`
- ‚ùå Queries returnerar "Jag hittar inte svaret i k√§llorna" eftersom ingen cache finns

## L√∂sning: Indexering i Prod

F√∂r att f√• prod att fungera som lokal beh√∂ver vi:

### Steg 1: Skapa en indexeringspipeline-funktion

**Skapa:** `rag/index_pipeline.py`

```python
"""Indexering pipeline f√∂r dokument fr√•n R2."""
from typing import List, Dict, Any
from rag.store import Store
from rag.index_store import save_index
from ingest.text_extractor import extract_text
from ingest.chunker import chunk_text
from rag.embeddings_client import EmbeddingsClient
from rank_bm25 import BM25Okapi
import numpy as np
from api.r2_client import s3_client, R2_BUCKET_NAME
import tempfile
import os

def index_document_from_r2(
    storage_key: str,
    workspace_id: str,
    document_id: str,
    filename: str,
) -> Dict[str, Any]:
    """
    Indexera ett dokument fr√•n R2.
    
    1. L√§s fil fr√•n R2 till temp-fil
    2. Extrahera text
    3. Chunk text
    4. Generera embeddings
    5. Spara till cache
    6. Uppdatera DB med chunks
    """
    # 1. L√§s fr√•n R2
    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filename)[1]) as tmp_file:
        s3_client.download_fileobj(R2_BUCKET_NAME, storage_key, tmp_file)
        tmp_path = tmp_file.name
    
    try:
        # 2. Extrahera text
        text, _ = extract_text(tmp_path)
        
        # 3. Chunk text
        chunks = chunk_text(text, target_tokens=512, overlap_tokens=50)
        
        # 4. Generera embeddings
        emb_client = EmbeddingsClient()
        chunk_texts = [c["text"] for c in chunks]
        embeddings = emb_client.embed_texts(chunk_texts)
        
        # 5. Spara till cache (l√§s befintlig cache och merge)
        from rag.index_store import load_index
        cfg = load_config()
        cache_dir = cfg.get("storage", {}).get("index_dir", "index_cache")
        
        existing_cache = load_index(workspace_id, cache_dir)
        
        if existing_cache:
            # Merge med befintlig cache
            existing_chunks = existing_cache["chunks_meta"]
            existing_embeddings = existing_cache["embeddings"]
            # ... merge logik ...
        else:
            # Ny cache
            chunks_meta = [
                {
                    "chunk_id": f"chunk-{i+1}",
                    "document_id": document_id,
                    "document_name": filename,
                    "text": chunk["text"],
                    "page_number": 1,
                    "workspace_id": workspace_id,
                }
                for i, chunk in enumerate(chunks)
            ]
            
            save_index(
                workspace=workspace_id,
                embeddings=np.vstack([np.array(e, dtype=float) for e in embeddings]),
                chunks_meta=chunks_meta,
                bm25_obj=BM25Okapi([t.split() for t in chunk_texts]),
                base_dir=cache_dir,
            )
        
        # 6. Spara chunks till DB
        store = Store()
        # ... spara chunks ...
        
        return {
            "success": True,
            "chunks": len(chunks),
            "document_id": document_id,
        }
    finally:
        # Rensa temp-fil
        os.unlink(tmp_path)
```

### Steg 2: Integrera i `/documents/upload`

**I `api/main.py` i `upload_document`:**

```python
# Efter att dokument √§r sparat i R2 och DB:
# TODO: Indexera dokumentet fr√•n R2
try:
    from rag.index_pipeline import index_document_from_r2
    index_result = index_document_from_r2(
        storage_key=storage_key,
        workspace_id=str(user_id),  # Eller r√§tt workspace
        document_id=str(document_data["id"]),
        filename=filename,
    )
    print(f"[upload] Indexerade dokument: {index_result['chunks']} chunks")
except Exception as e:
    print(f"[upload] VARNING: Kunde inte indexera dokument: {e}")
    # Forts√§tt √§nd√•, dokumentet √§r sparat i R2/DB
```

### Steg 3: Alternativ: Async indexing

Om indexering tar f√∂r l√•ng tid kan vi k√∂ra det asynkront:

```python
# I upload_document, efter att dokument √§r sparat:
import asyncio
asyncio.create_task(index_document_async(storage_key, workspace_id, document_id, filename))
```

### Steg 4: Invalidera cached engine

Efter indexering m√•ste vi invalidera cached engine s√• den laddas om:

```python
# I api/main.py, efter indexering:
global _engines
if workspace_id in _engines:
    del _engines[workspace_id]  # Force reload n√§sta query
```

## N√§sta steg

1. ‚úÖ Fixa `verbose_mode`-buggen (gjort!)
2. ‚è≥ Skapa `rag/index_pipeline.py` med indexeringslogik
3. ‚è≥ Integrera i `/documents/upload`
4. ‚è≥ Testa i prod
5. ‚è≥ Verifiera att `/health` visar `indexed_chunks > 0`

## Test

Efter implementation:

1. Ladda upp ett dokument via sintari.se
2. Kolla Railway logs f√∂r `[upload] Indexerade dokument: X chunks`
3. Testa `/health` - ska visa `indexed_chunks > 0`
4. St√§ll en fr√•ga i chatten - ska hitta dokumentet!

## Status

- ‚úÖ `verbose_mode`-bug fixad
- ‚è≥ Indexering i prod - TODO (se ovan)

